{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset"
      ],
      "metadata": {
        "id": "sDZrCwu_SRwa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpHdMAvmjCxj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install wandb\n",
        "!tar -xvf /content/drive/MyDrive/work/Dysarthria_VIVO_system/data/UASpeech.tar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "from os import walk\n",
        "import librosa\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoFeatureExtractor, AutoModel, HubertModel, Wav2Vec2FeatureExtractor"
      ],
      "metadata": {
        "id": "TDSCutzJB8X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoWytp4QyocL",
        "outputId": "9c7b8593-657a-432b-81b9-0afbbd040ecf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmukhtaralgezoli\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# setup wandb\n",
        "wandb_API_key = \"af0ebd78dadd977aadb9b94cc811dc60924219fc\"\n",
        "wandb.login(key = wandb_API_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUzOdU5EPe54",
        "outputId": "5721dff8-d3e7-47d6-ba05-b9cbf1a69703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# specify device for training\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9jJxrVbXidm"
      },
      "outputs": [],
      "source": [
        "class UASpeechDataset(Dataset):\n",
        "    def __init__(self, UASpeech_metadata_path, model_path, transform=None, target_transform=None):\n",
        "        self.metadata = pd.read_csv(UASpeech_metadata_path)\n",
        "        self.processor = AutoFeatureExtractor.from_pretrained(model_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.metadata.loc[self.metadata.index[idx], 'path']\n",
        "        speech, samplerate = sf.read(audio_path)\n",
        "        # seconds = librosa.get_duration(path= audio_path)\n",
        "        # print(seconds)\n",
        "        speech = speech[0:250000]\n",
        "\n",
        "        preprocessed_speech = self.processor(speech, padding=\"max_length\",  max_length = 250000,return_tensors=\"pt\", sampling_rate = 16000).input_values\n",
        "        # label = self.metadata.iloc[idx, 3]\n",
        "        label = self.metadata.loc[self.metadata.index[idx], \"Intelligibility_Label_id\"]\n",
        "\n",
        "        return preprocessed_speech, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtyWF7AsZYLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee948f0a-dc3e-439b-b079-413355b53c30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_set = UASpeechDataset(\"/content/drive/MyDrive/work/Dysarthria_VIVO_system/data/train_df_4labels.csv\", \"facebook/hubert-base-ls960\")\n",
        "test_set = UASpeechDataset(\"/content/drive/MyDrive/work/Dysarthria_VIVO_system/data/test_df_4labels.csv\", \"facebook/hubert-base-ls960\")\n",
        "val_set = UASpeechDataset(\"/content/drive/MyDrive/work/Dysarthria_VIVO_system/data/val_df_4labels.csv\", \"facebook/hubert-base-ls960\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a6P0NaepznS"
      },
      "outputs": [],
      "source": [
        "batch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCJpiHRdZgd8"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_set, batch_size = batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_set, batch_size = batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_set, batch_size = batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gup4BVeXTbnJ"
      },
      "source": [
        "# Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-wcivHlenw3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoFeatureExtractor, AutoModel, HubertModel, Wav2Vec2FeatureExtractor\n",
        "import soundfile as sf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiCuzwzrg25-"
      },
      "outputs": [],
      "source": [
        "# Classification Head\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, hidden_size, num_labels):\n",
        "        super().__init__()\n",
        "        self.linearLayer = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out_proj = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, features):\n",
        "        x = features\n",
        "        x = self.linearLayer(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.out_proj(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmgKBKB3faZn"
      },
      "outputs": [],
      "source": [
        "# Model class\n",
        "class Dysarthria_model(nn.Module):\n",
        "  def __init__(self, model_path = \"facebook/hubert-base-ls960\", pooling_mode = \"mean\", num_output_labels = 4):\n",
        "        super().__init__()\n",
        "        # self.processor = AutoFeatureExtractor.from_pretrained(model_path)\n",
        "        self.SSLModel = AutoModel.from_pretrained(model_path)\n",
        "        self.classificationHead = ClassificationHead(768, num_output_labels)\n",
        "        self.pooling_mode = pooling_mode\n",
        "\n",
        "  def freeze_feature_extractor(self):\n",
        "        self.SSLModel.feature_extractor._freeze_parameters()\n",
        "\n",
        "  def merged_strategy(self, output_features, mode=\"mean\"):\n",
        "        if mode == \"mean\":\n",
        "            outputs = torch.mean(output_features, dim=1)\n",
        "        elif mode == \"sum\":\n",
        "            outputs = torch.sum(output_features, dim=1)\n",
        "        elif mode == \"max\":\n",
        "            outputs = torch.max(output_features, dim=1)[0]\n",
        "        else:\n",
        "            raise Exception(\n",
        "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
        "\n",
        "        return outputs\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "\n",
        "    output_features = self.SSLModel(x).last_hidden_state\n",
        "    hidden_states = self.merged_strategy(output_features, mode=self.pooling_mode)\n",
        "    logits = self.classificationHead(hidden_states)\n",
        "\n",
        "    return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj4_K0rIfbcw",
        "outputId": "a930bbca-a56b-4b74-e092-717b0e4a2675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/hubert-base-ls960 were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = Dysarthria_model(model_path = \"facebook/hubert-base-ls960\").to(device)\n",
        "model.freeze_feature_extractor()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint = torch.load(\"/content/drive/MyDrive/work/Emotions_SER_project/Saved Models/EXP4/Hubert/Checkpoint_Epoch23.pt\")\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# # epoch = checkpoint['epoch']\n",
        "# # loss = checkpoint['loss']"
      ],
      "metadata": {
        "id": "k2P44JQO1_6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULXgjWXFZ_k2",
        "outputId": "9486538a-afad-4954-80ee-38d4df38fe68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4])\n"
          ]
        }
      ],
      "source": [
        "# Test model\n",
        "preprocessed_speech_sample, label = train_set[1]\n",
        "output = model(preprocessed_speech_sample.to(device))\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZpmJdBKkGRz"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM6z_JMsLPyE"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "r5inAz6Xz8dk",
        "outputId": "0c5902ec-d19a-430d-8ca6-c6d27ef2c6bc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240220_103651-qehrpyfo</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mukhtaralgezoli/Dysarthria_classification/runs/qehrpyfo' target=\"_blank\">test-run</a></strong> to <a href='https://wandb.ai/mukhtaralgezoli/Dysarthria_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mukhtaralgezoli/Dysarthria_classification' target=\"_blank\">https://wandb.ai/mukhtaralgezoli/Dysarthria_classification</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mukhtaralgezoli/Dysarthria_classification/runs/qehrpyfo' target=\"_blank\">https://wandb.ai/mukhtaralgezoli/Dysarthria_classification/runs/qehrpyfo</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mukhtaralgezoli/Dysarthria_classification/runs/qehrpyfo?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7de231f20be0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# start a new wandb run to track this script\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"Dysarthria_classification\",\n",
        "    # id=\"pwim79rh\",\n",
        "    # resume=\"must\",\n",
        "    name = \"test-run\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"architecture\": \"Hubert for classification\",\n",
        "    \"dataset\": \"UASpeech\",\n",
        "    \"epochs\": 40,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_id = wandb.run.id\n",
        "run_name = wandb.run.name\n",
        "print(run_id)\n",
        "print(run_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnrdBzdnB_Jf",
        "outputId": "f4d12210-9a3e-4bce-f243-21ac85c985fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qehrpyfo\n",
            "test-run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWZ0zaaOLP7E"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    model.train()\n",
        "    train_loss, correct = 0, 0\n",
        "\n",
        "    for batch, (batch_input, batch_labels) in enumerate(dataloader):\n",
        "      batch_input = torch.squeeze(batch_input, 1)\n",
        "      pred = model(batch_input.to(device))\n",
        "      loss = loss_fn(pred, batch_labels.to(device))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      train_loss += loss_fn(pred, batch_labels.to(device)).item()\n",
        "      correct += (pred.argmax(1) == batch_labels.to(device)).type(torch.float).sum().item()\n",
        "\n",
        "      if batch % 5 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(batch_input)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    train_loss /= num_batches\n",
        "    correct /= size\n",
        "\n",
        "    wandb.log({\"train loss\": train_loss})\n",
        "    wandb.log({\"train accuracy\": 100*correct})\n",
        "    return train_loss, 100*correct\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_input, batch_labels in dataloader:\n",
        "            batch_input = torch.squeeze(batch_input, 1)\n",
        "            pred = model(batch_input.to(device))\n",
        "            test_loss += loss_fn(pred, batch_labels.to(device)).item()\n",
        "            correct += (pred.argmax(1) == batch_labels.to(device)).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "\n",
        "    wandb.log({\"val loss\": test_loss})\n",
        "    wandb.log({\"val accuracy\": 100*correct})\n",
        "\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss, 100*correct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBoJ7pKHsbDq",
        "outputId": "6815c1b9-f5d2-43c7-f58c-88a66b1bd83c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.387804  [    8/ 7105]\n",
            "loss: 1.406658  [   48/ 7105]\n",
            "loss: 1.404901  [   88/ 7105]\n",
            "loss: 1.397587  [  128/ 7105]\n",
            "loss: 1.357024  [  168/ 7105]\n",
            "loss: 1.322815  [  208/ 7105]\n",
            "loss: 1.370412  [  248/ 7105]\n",
            "loss: 1.346765  [  288/ 7105]\n",
            "loss: 1.302205  [  328/ 7105]\n",
            "loss: 1.279243  [  368/ 7105]\n",
            "loss: 1.331513  [  408/ 7105]\n",
            "loss: 1.419997  [  448/ 7105]\n",
            "loss: 1.382215  [  488/ 7105]\n",
            "loss: 1.274992  [  528/ 7105]\n",
            "loss: 1.351652  [  568/ 7105]\n",
            "loss: 1.361107  [  608/ 7105]\n",
            "loss: 1.221396  [  648/ 7105]\n",
            "loss: 1.353175  [  688/ 7105]\n",
            "loss: 1.252706  [  728/ 7105]\n",
            "loss: 1.260732  [  768/ 7105]\n",
            "loss: 1.293949  [  808/ 7105]\n",
            "loss: 1.268990  [  848/ 7105]\n",
            "loss: 1.203893  [  888/ 7105]\n",
            "loss: 1.354618  [  928/ 7105]\n",
            "loss: 1.245376  [  968/ 7105]\n",
            "loss: 1.439895  [ 1008/ 7105]\n",
            "loss: 1.456529  [ 1048/ 7105]\n",
            "loss: 1.309897  [ 1088/ 7105]\n",
            "loss: 1.466358  [ 1128/ 7105]\n",
            "loss: 1.449846  [ 1168/ 7105]\n",
            "loss: 1.347154  [ 1208/ 7105]\n",
            "loss: 1.318473  [ 1248/ 7105]\n",
            "loss: 1.339964  [ 1288/ 7105]\n",
            "loss: 1.381312  [ 1328/ 7105]\n",
            "loss: 1.279316  [ 1368/ 7105]\n",
            "loss: 1.295372  [ 1408/ 7105]\n",
            "loss: 1.341880  [ 1448/ 7105]\n",
            "loss: 1.313586  [ 1488/ 7105]\n",
            "loss: 1.403836  [ 1528/ 7105]\n",
            "loss: 1.427511  [ 1568/ 7105]\n",
            "loss: 1.448796  [ 1608/ 7105]\n",
            "loss: 1.290227  [ 1648/ 7105]\n",
            "loss: 1.326255  [ 1688/ 7105]\n",
            "loss: 1.282181  [ 1728/ 7105]\n",
            "loss: 1.251722  [ 1768/ 7105]\n",
            "loss: 1.230586  [ 1808/ 7105]\n",
            "loss: 1.128504  [ 1848/ 7105]\n",
            "loss: 1.394827  [ 1888/ 7105]\n",
            "loss: 1.413369  [ 1928/ 7105]\n",
            "loss: 1.612273  [ 1968/ 7105]\n",
            "loss: 1.401553  [ 2008/ 7105]\n",
            "loss: 1.236078  [ 2048/ 7105]\n",
            "loss: 1.385414  [ 2088/ 7105]\n",
            "loss: 1.254063  [ 2128/ 7105]\n",
            "loss: 1.456931  [ 2168/ 7105]\n",
            "loss: 1.417607  [ 2208/ 7105]\n",
            "loss: 1.290092  [ 2248/ 7105]\n",
            "loss: 1.307855  [ 2288/ 7105]\n",
            "loss: 1.315119  [ 2328/ 7105]\n",
            "loss: 1.544415  [ 2368/ 7105]\n",
            "loss: 1.296785  [ 2408/ 7105]\n",
            "loss: 1.154646  [ 2448/ 7105]\n",
            "loss: 1.255872  [ 2488/ 7105]\n",
            "loss: 1.246957  [ 2528/ 7105]\n",
            "loss: 1.336975  [ 2568/ 7105]\n",
            "loss: 1.318287  [ 2608/ 7105]\n",
            "loss: 1.358606  [ 2648/ 7105]\n",
            "loss: 1.337734  [ 2688/ 7105]\n",
            "loss: 1.034609  [ 2728/ 7105]\n",
            "loss: 1.302935  [ 2768/ 7105]\n",
            "loss: 1.263183  [ 2808/ 7105]\n",
            "loss: 1.407136  [ 2848/ 7105]\n",
            "loss: 1.235514  [ 2888/ 7105]\n",
            "loss: 1.294775  [ 2928/ 7105]\n",
            "loss: 1.143904  [ 2968/ 7105]\n",
            "loss: 1.447087  [ 3008/ 7105]\n",
            "loss: 1.166713  [ 3048/ 7105]\n",
            "loss: 1.167492  [ 3088/ 7105]\n",
            "loss: 1.125301  [ 3128/ 7105]\n",
            "loss: 1.368364  [ 3168/ 7105]\n",
            "loss: 1.310626  [ 3208/ 7105]\n",
            "loss: 1.148139  [ 3248/ 7105]\n",
            "loss: 1.158317  [ 3288/ 7105]\n",
            "loss: 1.230046  [ 3328/ 7105]\n",
            "loss: 1.344027  [ 3368/ 7105]\n",
            "loss: 1.244685  [ 3408/ 7105]\n",
            "loss: 1.235321  [ 3448/ 7105]\n",
            "loss: 1.330514  [ 3488/ 7105]\n",
            "loss: 1.145120  [ 3528/ 7105]\n",
            "loss: 1.340821  [ 3568/ 7105]\n",
            "loss: 1.063864  [ 3608/ 7105]\n",
            "loss: 1.183975  [ 3648/ 7105]\n",
            "loss: 1.226489  [ 3688/ 7105]\n",
            "loss: 1.278914  [ 3728/ 7105]\n",
            "loss: 1.114574  [ 3768/ 7105]\n",
            "loss: 1.327780  [ 3808/ 7105]\n",
            "loss: 1.162311  [ 3848/ 7105]\n",
            "loss: 1.164587  [ 3888/ 7105]\n",
            "loss: 1.232494  [ 3928/ 7105]\n",
            "loss: 1.419406  [ 3968/ 7105]\n",
            "loss: 1.246472  [ 4008/ 7105]\n",
            "loss: 1.171959  [ 4048/ 7105]\n",
            "loss: 1.342967  [ 4088/ 7105]\n",
            "loss: 1.174871  [ 4128/ 7105]\n",
            "loss: 1.323172  [ 4168/ 7105]\n",
            "loss: 1.272457  [ 4208/ 7105]\n",
            "loss: 1.240503  [ 4248/ 7105]\n",
            "loss: 1.422247  [ 4288/ 7105]\n",
            "loss: 1.145498  [ 4328/ 7105]\n",
            "loss: 1.261824  [ 4368/ 7105]\n",
            "loss: 1.423268  [ 4408/ 7105]\n",
            "loss: 1.099435  [ 4448/ 7105]\n",
            "loss: 1.388142  [ 4488/ 7105]\n",
            "loss: 1.182516  [ 4528/ 7105]\n",
            "loss: 1.261240  [ 4568/ 7105]\n",
            "loss: 1.342284  [ 4608/ 7105]\n",
            "loss: 1.551244  [ 4648/ 7105]\n",
            "loss: 1.157879  [ 4688/ 7105]\n",
            "loss: 1.130988  [ 4728/ 7105]\n",
            "loss: 0.942875  [ 4768/ 7105]\n",
            "loss: 1.448254  [ 4808/ 7105]\n",
            "loss: 1.295988  [ 4848/ 7105]\n",
            "loss: 1.358742  [ 4888/ 7105]\n",
            "loss: 1.260409  [ 4928/ 7105]\n",
            "loss: 1.325939  [ 4968/ 7105]\n",
            "loss: 1.114279  [ 5008/ 7105]\n",
            "loss: 1.089711  [ 5048/ 7105]\n",
            "loss: 1.620914  [ 5088/ 7105]\n",
            "loss: 1.090756  [ 5128/ 7105]\n",
            "loss: 1.184442  [ 5168/ 7105]\n",
            "loss: 1.260819  [ 5208/ 7105]\n",
            "loss: 1.215406  [ 5248/ 7105]\n",
            "loss: 1.054357  [ 5288/ 7105]\n",
            "loss: 1.161612  [ 5328/ 7105]\n",
            "loss: 1.209342  [ 5368/ 7105]\n",
            "loss: 1.170567  [ 5408/ 7105]\n",
            "loss: 1.319257  [ 5448/ 7105]\n",
            "loss: 0.884820  [ 5488/ 7105]\n",
            "loss: 1.336674  [ 5528/ 7105]\n",
            "loss: 1.307284  [ 5568/ 7105]\n",
            "loss: 1.166327  [ 5608/ 7105]\n",
            "loss: 1.319698  [ 5648/ 7105]\n",
            "loss: 1.343504  [ 5688/ 7105]\n",
            "loss: 0.988590  [ 5728/ 7105]\n",
            "loss: 1.202955  [ 5768/ 7105]\n",
            "loss: 1.452424  [ 5808/ 7105]\n",
            "loss: 1.179457  [ 5848/ 7105]\n",
            "loss: 1.269805  [ 5888/ 7105]\n",
            "loss: 1.121287  [ 5928/ 7105]\n",
            "loss: 1.333482  [ 5968/ 7105]\n",
            "loss: 1.333250  [ 6008/ 7105]\n",
            "loss: 1.061133  [ 6048/ 7105]\n",
            "loss: 1.182604  [ 6088/ 7105]\n",
            "loss: 1.160173  [ 6128/ 7105]\n",
            "loss: 1.424453  [ 6168/ 7105]\n",
            "loss: 1.084448  [ 6208/ 7105]\n",
            "loss: 0.872305  [ 6248/ 7105]\n",
            "loss: 1.110671  [ 6288/ 7105]\n",
            "loss: 1.048890  [ 6328/ 7105]\n",
            "loss: 1.235846  [ 6368/ 7105]\n",
            "loss: 1.263230  [ 6408/ 7105]\n",
            "loss: 1.164263  [ 6448/ 7105]\n",
            "loss: 1.020085  [ 6488/ 7105]\n",
            "loss: 0.981753  [ 6528/ 7105]\n",
            "loss: 1.000698  [ 6568/ 7105]\n",
            "loss: 1.293847  [ 6608/ 7105]\n",
            "loss: 1.262551  [ 6648/ 7105]\n",
            "loss: 0.817867  [ 6688/ 7105]\n",
            "loss: 1.017904  [ 6728/ 7105]\n",
            "loss: 1.183437  [ 6768/ 7105]\n",
            "loss: 1.150382  [ 6808/ 7105]\n",
            "loss: 0.824257  [ 6848/ 7105]\n",
            "loss: 1.201856  [ 6888/ 7105]\n",
            "loss: 0.867808  [ 6928/ 7105]\n",
            "loss: 1.080284  [ 6968/ 7105]\n",
            "loss: 1.246562  [ 7008/ 7105]\n",
            "loss: 0.759574  [ 7048/ 7105]\n",
            "loss: 1.075056  [ 7088/ 7105]\n",
            "Test Error: \n",
            " Accuracy: 45.8%, Avg loss: 1.229084 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.137329  [    8/ 7105]\n",
            "loss: 0.999250  [   48/ 7105]\n",
            "loss: 1.355384  [   88/ 7105]\n",
            "loss: 1.219556  [  128/ 7105]\n",
            "loss: 1.065851  [  168/ 7105]\n",
            "loss: 0.916681  [  208/ 7105]\n",
            "loss: 1.232680  [  248/ 7105]\n",
            "loss: 0.982868  [  288/ 7105]\n",
            "loss: 1.066521  [  328/ 7105]\n",
            "loss: 0.670434  [  368/ 7105]\n",
            "loss: 0.812778  [  408/ 7105]\n",
            "loss: 0.742568  [  448/ 7105]\n",
            "loss: 1.056031  [  488/ 7105]\n",
            "loss: 1.305542  [  528/ 7105]\n",
            "loss: 0.744874  [  568/ 7105]\n",
            "loss: 0.990758  [  608/ 7105]\n",
            "loss: 0.918704  [  648/ 7105]\n",
            "loss: 1.136198  [  688/ 7105]\n",
            "loss: 0.849435  [  728/ 7105]\n",
            "loss: 1.313402  [  768/ 7105]\n",
            "loss: 1.217188  [  808/ 7105]\n",
            "loss: 1.371600  [  848/ 7105]\n",
            "loss: 1.443387  [  888/ 7105]\n",
            "loss: 0.743095  [  928/ 7105]\n",
            "loss: 1.100141  [  968/ 7105]\n",
            "loss: 0.876635  [ 1008/ 7105]\n",
            "loss: 1.271078  [ 1048/ 7105]\n",
            "loss: 0.955729  [ 1088/ 7105]\n",
            "loss: 0.982498  [ 1128/ 7105]\n",
            "loss: 0.879460  [ 1168/ 7105]\n",
            "loss: 1.102803  [ 1208/ 7105]\n",
            "loss: 0.945227  [ 1248/ 7105]\n",
            "loss: 1.202041  [ 1288/ 7105]\n",
            "loss: 0.851958  [ 1328/ 7105]\n",
            "loss: 1.116287  [ 1368/ 7105]\n",
            "loss: 0.829534  [ 1408/ 7105]\n",
            "loss: 1.235356  [ 1448/ 7105]\n",
            "loss: 1.409032  [ 1488/ 7105]\n",
            "loss: 1.153315  [ 1528/ 7105]\n",
            "loss: 1.262134  [ 1568/ 7105]\n",
            "loss: 1.231872  [ 1608/ 7105]\n",
            "loss: 0.848087  [ 1648/ 7105]\n",
            "loss: 0.688516  [ 1688/ 7105]\n",
            "loss: 0.956145  [ 1728/ 7105]\n",
            "loss: 1.363212  [ 1768/ 7105]\n",
            "loss: 0.759721  [ 1808/ 7105]\n",
            "loss: 1.102331  [ 1848/ 7105]\n",
            "loss: 0.937350  [ 1888/ 7105]\n",
            "loss: 0.902369  [ 1928/ 7105]\n",
            "loss: 1.189467  [ 1968/ 7105]\n",
            "loss: 0.877322  [ 2008/ 7105]\n",
            "loss: 0.696956  [ 2048/ 7105]\n",
            "loss: 0.812937  [ 2088/ 7105]\n",
            "loss: 1.132955  [ 2128/ 7105]\n",
            "loss: 1.065396  [ 2168/ 7105]\n",
            "loss: 1.329370  [ 2208/ 7105]\n",
            "loss: 0.918267  [ 2248/ 7105]\n",
            "loss: 1.063298  [ 2288/ 7105]\n",
            "loss: 0.994845  [ 2328/ 7105]\n",
            "loss: 0.663521  [ 2368/ 7105]\n",
            "loss: 0.417071  [ 2408/ 7105]\n",
            "loss: 1.248632  [ 2448/ 7105]\n",
            "loss: 1.174971  [ 2488/ 7105]\n",
            "loss: 0.562097  [ 2528/ 7105]\n",
            "loss: 0.508242  [ 2568/ 7105]\n",
            "loss: 1.349683  [ 2608/ 7105]\n",
            "loss: 0.802582  [ 2648/ 7105]\n",
            "loss: 1.248652  [ 2688/ 7105]\n",
            "loss: 0.755051  [ 2728/ 7105]\n",
            "loss: 1.212278  [ 2768/ 7105]\n",
            "loss: 1.320832  [ 2808/ 7105]\n",
            "loss: 0.960854  [ 2848/ 7105]\n",
            "loss: 0.997510  [ 2888/ 7105]\n",
            "loss: 0.910945  [ 2928/ 7105]\n",
            "loss: 1.407585  [ 2968/ 7105]\n",
            "loss: 1.091192  [ 3008/ 7105]\n",
            "loss: 1.161580  [ 3048/ 7105]\n",
            "loss: 0.872673  [ 3088/ 7105]\n",
            "loss: 0.779752  [ 3128/ 7105]\n",
            "loss: 0.938567  [ 3168/ 7105]\n",
            "loss: 0.951877  [ 3208/ 7105]\n",
            "loss: 0.820725  [ 3248/ 7105]\n",
            "loss: 1.349179  [ 3288/ 7105]\n",
            "loss: 0.844267  [ 3328/ 7105]\n",
            "loss: 0.413449  [ 3368/ 7105]\n",
            "loss: 1.229169  [ 3408/ 7105]\n",
            "loss: 0.763696  [ 3448/ 7105]\n",
            "loss: 1.105219  [ 3488/ 7105]\n",
            "loss: 0.943968  [ 3528/ 7105]\n",
            "loss: 0.808342  [ 3568/ 7105]\n",
            "loss: 1.085635  [ 3608/ 7105]\n",
            "loss: 0.720805  [ 3648/ 7105]\n",
            "loss: 0.922679  [ 3688/ 7105]\n",
            "loss: 0.773737  [ 3728/ 7105]\n",
            "loss: 0.796093  [ 3768/ 7105]\n",
            "loss: 0.509203  [ 3808/ 7105]\n",
            "loss: 0.701203  [ 3848/ 7105]\n",
            "loss: 1.033431  [ 3888/ 7105]\n",
            "loss: 0.546935  [ 3928/ 7105]\n",
            "loss: 0.845895  [ 3968/ 7105]\n",
            "loss: 0.593973  [ 4008/ 7105]\n",
            "loss: 1.198649  [ 4048/ 7105]\n",
            "loss: 0.743100  [ 4088/ 7105]\n",
            "loss: 0.929046  [ 4128/ 7105]\n",
            "loss: 0.817559  [ 4168/ 7105]\n",
            "loss: 0.671733  [ 4208/ 7105]\n",
            "loss: 0.738185  [ 4248/ 7105]\n",
            "loss: 0.766689  [ 4288/ 7105]\n",
            "loss: 1.066690  [ 4328/ 7105]\n",
            "loss: 0.825540  [ 4368/ 7105]\n",
            "loss: 1.191199  [ 4408/ 7105]\n",
            "loss: 0.800315  [ 4448/ 7105]\n",
            "loss: 0.673532  [ 4488/ 7105]\n",
            "loss: 0.256079  [ 4528/ 7105]\n",
            "loss: 1.056892  [ 4568/ 7105]\n",
            "loss: 1.139681  [ 4608/ 7105]\n",
            "loss: 0.659614  [ 4648/ 7105]\n",
            "loss: 0.881048  [ 4688/ 7105]\n",
            "loss: 1.674970  [ 4728/ 7105]\n",
            "loss: 0.936912  [ 4768/ 7105]\n",
            "loss: 0.861191  [ 4808/ 7105]\n",
            "loss: 0.502734  [ 4848/ 7105]\n",
            "loss: 1.097643  [ 4888/ 7105]\n",
            "loss: 1.579168  [ 4928/ 7105]\n",
            "loss: 1.205141  [ 4968/ 7105]\n",
            "loss: 0.988609  [ 5008/ 7105]\n",
            "loss: 1.004684  [ 5048/ 7105]\n",
            "loss: 0.719944  [ 5088/ 7105]\n",
            "loss: 0.730037  [ 5128/ 7105]\n",
            "loss: 0.861298  [ 5168/ 7105]\n",
            "loss: 0.828675  [ 5208/ 7105]\n",
            "loss: 0.698507  [ 5248/ 7105]\n",
            "loss: 1.107142  [ 5288/ 7105]\n",
            "loss: 1.243253  [ 5328/ 7105]\n",
            "loss: 0.723161  [ 5368/ 7105]\n",
            "loss: 0.706959  [ 5408/ 7105]\n",
            "loss: 1.125711  [ 5448/ 7105]\n",
            "loss: 0.586967  [ 5488/ 7105]\n",
            "loss: 0.325107  [ 5528/ 7105]\n",
            "loss: 0.780393  [ 5568/ 7105]\n",
            "loss: 1.136733  [ 5608/ 7105]\n",
            "loss: 0.585084  [ 5648/ 7105]\n",
            "loss: 1.102719  [ 5688/ 7105]\n",
            "loss: 0.754015  [ 5728/ 7105]\n",
            "loss: 0.675442  [ 5768/ 7105]\n",
            "loss: 1.083887  [ 5808/ 7105]\n",
            "loss: 0.907828  [ 5848/ 7105]\n",
            "loss: 0.938273  [ 5888/ 7105]\n",
            "loss: 0.570184  [ 5928/ 7105]\n",
            "loss: 0.869522  [ 5968/ 7105]\n",
            "loss: 0.529218  [ 6008/ 7105]\n",
            "loss: 1.169492  [ 6048/ 7105]\n",
            "loss: 0.948690  [ 6088/ 7105]\n",
            "loss: 1.039582  [ 6128/ 7105]\n",
            "loss: 1.150874  [ 6168/ 7105]\n",
            "loss: 0.465812  [ 6208/ 7105]\n",
            "loss: 0.615763  [ 6248/ 7105]\n",
            "loss: 0.604830  [ 6288/ 7105]\n",
            "loss: 0.970382  [ 6328/ 7105]\n",
            "loss: 0.929620  [ 6368/ 7105]\n",
            "loss: 0.521802  [ 6408/ 7105]\n",
            "loss: 0.759187  [ 6448/ 7105]\n",
            "loss: 1.035053  [ 6488/ 7105]\n",
            "loss: 0.983896  [ 6528/ 7105]\n",
            "loss: 0.861618  [ 6568/ 7105]\n",
            "loss: 0.683163  [ 6608/ 7105]\n",
            "loss: 0.707307  [ 6648/ 7105]\n",
            "loss: 1.210979  [ 6688/ 7105]\n",
            "loss: 0.568275  [ 6728/ 7105]\n",
            "loss: 0.720910  [ 6768/ 7105]\n",
            "loss: 0.484819  [ 6808/ 7105]\n",
            "loss: 0.440012  [ 6848/ 7105]\n",
            "loss: 0.541059  [ 6888/ 7105]\n",
            "loss: 0.261282  [ 6928/ 7105]\n",
            "loss: 0.857265  [ 6968/ 7105]\n",
            "loss: 1.005323  [ 7008/ 7105]\n",
            "loss: 0.539699  [ 7048/ 7105]\n",
            "loss: 0.701816  [ 7088/ 7105]\n",
            "Test Error: \n",
            " Accuracy: 43.7%, Avg loss: 1.644562 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.282905  [    8/ 7105]\n",
            "loss: 0.353110  [   48/ 7105]\n",
            "loss: 0.585483  [   88/ 7105]\n",
            "loss: 0.811999  [  128/ 7105]\n",
            "loss: 0.985743  [  168/ 7105]\n",
            "loss: 0.441507  [  208/ 7105]\n",
            "loss: 0.491641  [  248/ 7105]\n",
            "loss: 0.617380  [  288/ 7105]\n",
            "loss: 0.331607  [  328/ 7105]\n",
            "loss: 0.722690  [  368/ 7105]\n",
            "loss: 0.655568  [  408/ 7105]\n",
            "loss: 0.777602  [  448/ 7105]\n",
            "loss: 0.807116  [  488/ 7105]\n",
            "loss: 0.705264  [  528/ 7105]\n",
            "loss: 0.309561  [  568/ 7105]\n",
            "loss: 0.821212  [  608/ 7105]\n",
            "loss: 0.747918  [  648/ 7105]\n",
            "loss: 0.486311  [  688/ 7105]\n",
            "loss: 0.730937  [  728/ 7105]\n",
            "loss: 0.620882  [  768/ 7105]\n",
            "loss: 0.856014  [  808/ 7105]\n",
            "loss: 1.206538  [  848/ 7105]\n",
            "loss: 0.586361  [  888/ 7105]\n",
            "loss: 0.680693  [  928/ 7105]\n",
            "loss: 0.337844  [  968/ 7105]\n",
            "loss: 0.703442  [ 1008/ 7105]\n",
            "loss: 1.048730  [ 1048/ 7105]\n",
            "loss: 0.899849  [ 1088/ 7105]\n",
            "loss: 1.077183  [ 1128/ 7105]\n",
            "loss: 0.704530  [ 1168/ 7105]\n",
            "loss: 0.552148  [ 1208/ 7105]\n",
            "loss: 0.642821  [ 1248/ 7105]\n",
            "loss: 0.710602  [ 1288/ 7105]\n",
            "loss: 0.504294  [ 1328/ 7105]\n",
            "loss: 0.815329  [ 1368/ 7105]\n",
            "loss: 0.990764  [ 1408/ 7105]\n",
            "loss: 0.465360  [ 1448/ 7105]\n",
            "loss: 0.882569  [ 1488/ 7105]\n",
            "loss: 0.646221  [ 1528/ 7105]\n",
            "loss: 0.505468  [ 1568/ 7105]\n",
            "loss: 0.959797  [ 1608/ 7105]\n",
            "loss: 0.984370  [ 1648/ 7105]\n",
            "loss: 0.518260  [ 1688/ 7105]\n",
            "loss: 0.784029  [ 1728/ 7105]\n",
            "loss: 0.663045  [ 1768/ 7105]\n",
            "loss: 0.494521  [ 1808/ 7105]\n",
            "loss: 0.896876  [ 1848/ 7105]\n",
            "loss: 0.559252  [ 1888/ 7105]\n",
            "loss: 0.901098  [ 1928/ 7105]\n",
            "loss: 0.477077  [ 1968/ 7105]\n",
            "loss: 0.535842  [ 2008/ 7105]\n",
            "loss: 0.793676  [ 2048/ 7105]\n",
            "loss: 1.026584  [ 2088/ 7105]\n",
            "loss: 0.908833  [ 2128/ 7105]\n",
            "loss: 0.915293  [ 2168/ 7105]\n",
            "loss: 0.677287  [ 2208/ 7105]\n",
            "loss: 0.640081  [ 2248/ 7105]\n",
            "loss: 0.507797  [ 2288/ 7105]\n",
            "loss: 1.059579  [ 2328/ 7105]\n",
            "loss: 0.816360  [ 2368/ 7105]\n",
            "loss: 0.810296  [ 2408/ 7105]\n",
            "loss: 0.782373  [ 2448/ 7105]\n",
            "loss: 1.037771  [ 2488/ 7105]\n",
            "loss: 0.512074  [ 2528/ 7105]\n",
            "loss: 0.865526  [ 2568/ 7105]\n",
            "loss: 0.845089  [ 2608/ 7105]\n",
            "loss: 0.878559  [ 2648/ 7105]\n",
            "loss: 0.910138  [ 2688/ 7105]\n",
            "loss: 1.236926  [ 2728/ 7105]\n",
            "loss: 0.743204  [ 2768/ 7105]\n",
            "loss: 0.510475  [ 2808/ 7105]\n",
            "loss: 0.577383  [ 2848/ 7105]\n",
            "loss: 0.413049  [ 2888/ 7105]\n",
            "loss: 0.439236  [ 2928/ 7105]\n",
            "loss: 0.361783  [ 2968/ 7105]\n",
            "loss: 0.415982  [ 3008/ 7105]\n",
            "loss: 0.737161  [ 3048/ 7105]\n",
            "loss: 0.906509  [ 3088/ 7105]\n",
            "loss: 0.344517  [ 3128/ 7105]\n",
            "loss: 0.808981  [ 3168/ 7105]\n",
            "loss: 0.750090  [ 3208/ 7105]\n",
            "loss: 1.234828  [ 3248/ 7105]\n",
            "loss: 0.693363  [ 3288/ 7105]\n",
            "loss: 0.620731  [ 3328/ 7105]\n",
            "loss: 0.645030  [ 3368/ 7105]\n",
            "loss: 0.624766  [ 3408/ 7105]\n",
            "loss: 1.202402  [ 3448/ 7105]\n",
            "loss: 0.878084  [ 3488/ 7105]\n",
            "loss: 0.639263  [ 3528/ 7105]\n",
            "loss: 0.310691  [ 3568/ 7105]\n",
            "loss: 0.608677  [ 3608/ 7105]\n",
            "loss: 0.568116  [ 3648/ 7105]\n",
            "loss: 0.770654  [ 3688/ 7105]\n",
            "loss: 0.787834  [ 3728/ 7105]\n",
            "loss: 0.715178  [ 3768/ 7105]\n",
            "loss: 0.224590  [ 3808/ 7105]\n",
            "loss: 0.664302  [ 3848/ 7105]\n",
            "loss: 0.617851  [ 3888/ 7105]\n",
            "loss: 0.184420  [ 3928/ 7105]\n",
            "loss: 0.552424  [ 3968/ 7105]\n",
            "loss: 1.047487  [ 4008/ 7105]\n",
            "loss: 0.845244  [ 4048/ 7105]\n",
            "loss: 1.303916  [ 4088/ 7105]\n",
            "loss: 0.605842  [ 4128/ 7105]\n",
            "loss: 0.429332  [ 4168/ 7105]\n",
            "loss: 0.126223  [ 4208/ 7105]\n",
            "loss: 0.520026  [ 4248/ 7105]\n",
            "loss: 0.548819  [ 4288/ 7105]\n",
            "loss: 0.459922  [ 4328/ 7105]\n",
            "loss: 0.729497  [ 4368/ 7105]\n",
            "loss: 0.292577  [ 4408/ 7105]\n",
            "loss: 0.800517  [ 4448/ 7105]\n",
            "loss: 0.515865  [ 4488/ 7105]\n",
            "loss: 0.723085  [ 4528/ 7105]\n",
            "loss: 0.614472  [ 4568/ 7105]\n",
            "loss: 0.631543  [ 4608/ 7105]\n",
            "loss: 1.191215  [ 4648/ 7105]\n",
            "loss: 0.756239  [ 4688/ 7105]\n",
            "loss: 0.502448  [ 4728/ 7105]\n",
            "loss: 0.580855  [ 4768/ 7105]\n",
            "loss: 1.236955  [ 4808/ 7105]\n",
            "loss: 0.647438  [ 4848/ 7105]\n",
            "loss: 0.932714  [ 4888/ 7105]\n",
            "loss: 0.386575  [ 4928/ 7105]\n",
            "loss: 0.563856  [ 4968/ 7105]\n",
            "loss: 0.983575  [ 5008/ 7105]\n",
            "loss: 0.843535  [ 5048/ 7105]\n",
            "loss: 1.235672  [ 5088/ 7105]\n",
            "loss: 0.531263  [ 5128/ 7105]\n",
            "loss: 0.337861  [ 5168/ 7105]\n",
            "loss: 0.911843  [ 5208/ 7105]\n",
            "loss: 0.983827  [ 5248/ 7105]\n",
            "loss: 0.726644  [ 5288/ 7105]\n",
            "loss: 0.565400  [ 5328/ 7105]\n",
            "loss: 0.990144  [ 5368/ 7105]\n",
            "loss: 0.501622  [ 5408/ 7105]\n",
            "loss: 0.710745  [ 5448/ 7105]\n",
            "loss: 0.597401  [ 5488/ 7105]\n",
            "loss: 0.266255  [ 5528/ 7105]\n",
            "loss: 0.286304  [ 5568/ 7105]\n",
            "loss: 1.169143  [ 5608/ 7105]\n",
            "loss: 1.234085  [ 5648/ 7105]\n",
            "loss: 0.657565  [ 5688/ 7105]\n",
            "loss: 0.675654  [ 5728/ 7105]\n",
            "loss: 0.759626  [ 5768/ 7105]\n",
            "loss: 0.572175  [ 5808/ 7105]\n",
            "loss: 0.331177  [ 5848/ 7105]\n",
            "loss: 0.723769  [ 5888/ 7105]\n",
            "loss: 0.954140  [ 5928/ 7105]\n",
            "loss: 0.241816  [ 5968/ 7105]\n",
            "loss: 0.659851  [ 6008/ 7105]\n",
            "loss: 1.039272  [ 6048/ 7105]\n",
            "loss: 0.511053  [ 6088/ 7105]\n",
            "loss: 0.615434  [ 6128/ 7105]\n",
            "loss: 0.878188  [ 6168/ 7105]\n",
            "loss: 0.621369  [ 6208/ 7105]\n",
            "loss: 0.545706  [ 6248/ 7105]\n",
            "loss: 0.253576  [ 6288/ 7105]\n",
            "loss: 0.769934  [ 6328/ 7105]\n",
            "loss: 0.487540  [ 6368/ 7105]\n",
            "loss: 0.599249  [ 6408/ 7105]\n",
            "loss: 0.515951  [ 6448/ 7105]\n",
            "loss: 0.828866  [ 6488/ 7105]\n",
            "loss: 1.508555  [ 6528/ 7105]\n",
            "loss: 0.666323  [ 6568/ 7105]\n",
            "loss: 0.731107  [ 6608/ 7105]\n",
            "loss: 0.980924  [ 6648/ 7105]\n",
            "loss: 0.380662  [ 6688/ 7105]\n",
            "loss: 1.179893  [ 6728/ 7105]\n",
            "loss: 0.316469  [ 6768/ 7105]\n",
            "loss: 0.850178  [ 6808/ 7105]\n",
            "loss: 0.567509  [ 6848/ 7105]\n",
            "loss: 0.594546  [ 6888/ 7105]\n",
            "loss: 0.353331  [ 6928/ 7105]\n",
            "loss: 0.357695  [ 6968/ 7105]\n",
            "loss: 0.340716  [ 7008/ 7105]\n",
            "loss: 0.808206  [ 7048/ 7105]\n",
            "loss: 0.169425  [ 7088/ 7105]\n",
            "Test Error: \n",
            " Accuracy: 79.1%, Avg loss: 0.518350 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.398083  [    8/ 7105]\n",
            "loss: 0.466004  [   48/ 7105]\n",
            "loss: 0.757783  [   88/ 7105]\n",
            "loss: 0.329908  [  128/ 7105]\n",
            "loss: 0.701566  [  168/ 7105]\n",
            "loss: 0.591877  [  208/ 7105]\n",
            "loss: 0.496683  [  248/ 7105]\n",
            "loss: 0.534353  [  288/ 7105]\n",
            "loss: 0.561460  [  328/ 7105]\n"
          ]
        }
      ],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "epochs = 40\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loss, train_acc = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    val_loss, val_acc = test_loop(val_dataloader, model, loss_fn)\n",
        "    torch.save({\n",
        "            'epoch': t,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train loss': train_loss,\n",
        "            'val loss': val_loss,\n",
        "            'train accuracy': train_acc,\n",
        "            'val accuracy': val_acc,\n",
        "            \"run Id\":run_id,\n",
        "            \"run name\":run_name,\n",
        "            }, f\"/content/drive/MyDrive/work/Dysarthria_VIVO_system/Saved Models/test_run/Hubert/Checkpoint_4labels_Epoch{t}.pt\")\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = test_loop(test_dataloader, model, loss_fn)"
      ],
      "metadata": {
        "id": "O6mgq-NUl4-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_acc = test_loop(val_dataloader, model, loss_fn)"
      ],
      "metadata": {
        "id": "iGK92U4YoeQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3snXTczOgDiE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}